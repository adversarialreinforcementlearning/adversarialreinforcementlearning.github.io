
<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="https://adversarialreinforcementlearning.github.io/" />
<meta property="og:url" content="https://adversarialreinforcementlearning.github.io/" />
<meta property="og:site_name" content="Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs" />
<script type="application/ld+json">
{"url":"https://adversarialreinforcementlearning.github.io/","@type":"WebSite","headline":"Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs","name":"Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=08f1fc066839e4123f8d359dd7f3c5bd6c5f8542">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
            m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-208543679-2', 'auto');
    ga('send', 'pageview');
  </script>



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      
      <h1><a href="https://adversarialreinforcementlearning.github.io/uai.html/">Investigating Vulnerabilities of Deep Neural Policies</a></h1>
      


<head>
  <meta name="keywords" content="adversarial, deep reinforcement learning, MDP, adversarial attacks, robust RL, safe RL, DeepRL, DRL, adversarial policies, robust reinforcement learning, AI safety, AI security, machine learning safety, adversarial machine learning, reinforcement learning, ML safety, ML security, adversarial reinforcement learning, robust reinforcement learning, adversarial RL, safe reinforcement learning, RL security, reinforcement learning security explainability, interpretability, AI alignment, machine learning explainability, ML interpretability, ">
</head>


<div align="center">
  Ezgi Korkmaz  <br />
  Conference on Uncertainty in Artificial Intelligence (UAI) 2021 <br />
  Published in Proceedings of Machine Learning Research (PMLR) 2021
  </div>


### Abstract



Reinforcement learning policies based on deep neural networks are vulnerable to imperceptible adversarial perturbations to their inputs, in much the same way as neural network image classifiers. Recent work has proposed several methods to improve the robustness of deep reinforcement learning agents to adversarial perturbations based on training in the presence of these imperceptible perturbations (i.e. adversarial training). In this paper, we study the effects of adversarial training on the neural policy learned by the agent. In particular, we follow two distinct parallel approaches to investigate the outcomes of adversarial training on deep neural policies based on worst-case distributional shift and feature sensitivity. For the first approach, we compare the Fourier spectrum of minimal perturbations computed for both adversarially trained and vanilla trained neural policies. Via experiments in the OpenAI Atari environments we show that minimal perturbations computed for adversarially trained policies are more focused on lower frequencies in the Fourier domain, indicating a higher sensitivity of these policies to low frequency perturbations. For the second approach, we propose a novel method to measure the feature sensitivities of deep neural policies and we compare these feature sensitivity differences in state-of-the-art adversarially trained deep neural policies and vanilla trained deep neural policies. We believe our results can be an initial step towards understanding the relationship between adversarial training and different notions of robustness for neural policies.


[[Paper]](https://proceedings.mlr.press/v161/korkmaz21a/korkmaz21a.pdf)   [[PMLR]](https://proceedings.mlr.press/v161/korkmaz21a.html)  [[Cite]](ekuaibibtex.html)


<br />
<br />

*Versions of this work presented in ICLR and ICML workshops and received Spotlight Presentation from NeurIPS workshop.


      
    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.1.0/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" crossorigin="anonymous"></script>
    <script>anchors.add();</script>
  </body>
</html>




